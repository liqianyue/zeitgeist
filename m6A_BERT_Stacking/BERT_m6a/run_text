cd examples
export KMER=6
export TRAIN_FILE=sample_data/pre/6_3k.txt
export TEST_FILE=sample_data/pre/6_3k.txt
export SOURCE=PATH_TO_DNABERT_REPO
export OUTPUT_PATH=output$KMER



python run_pretrain.py --output_dir output6 --model_type=dna --tokenizer_name=dna6 --config_name=../src/transformers/dnabert-config/bert-config-6/config.json --do_train --train_data_file=sample_data/pre/6_3k.txt --do_eval --eval_data_file=sample_data/pre/6_3k.txt --mlm --gradient_accumulation_steps 25 --per_gpu_train_batch_size 10 --per_gpu_eval_batch_size 6 --save_steps 500 --save_total_limit 20 --max_steps 200000 --evaluate_during_training --logging_steps 500 --line_by_line --learning_rate 4e-4 --block_size 512 --adam_epsilon 1e-6 --weight_decay 0.01 --beta1 0.9 --beta2 0.98 --mlm_probability 0.025 --warmup_steps 10000 --overwrite_output_dir --n_process 24

cd examples

export KMER=3
export MODEL_PATH=PATH_TO_THE_PRETRAINED_MODEL
export DATA_PATH=sample_data/ft/$KMER
export OUTPUT_PATH=./ft/$KMER

python run_finetune.py --model_type dna --tokenizer_name=dna6 --model_name_or_path output6 --task_name dnaprom --do_train --do_eval --data_dir sample_data/ft/DNN_data/6/h_b --max_seq_length 100 --per_gpu_eval_batch_size=10 --per_gpu_train_batch_size=10  --learning_rate 2e-4 --num_train_epochs 5.0 --output_dir ./ft/6_finetune --evaluate_during_training --logging_steps 400 --save_steps 400 --warmup_percent 0.1 --hidden_dropout_prob 0.1 --overwrite_output --weight_decay 0.01 --n_process 1


python run_finetune.py --model_type dna --tokenizer_name=dna6 --model_name_or_path output6 --task_name dnaprom --do_train --do_eval --data_dir sample_data/ft/m6a/6/A25 --max_seq_length 100 --per_gpu_eval_batch_size=10 --per_gpu_train_batch_size=10  --learning_rate 1e-4 --num_train_epochs 5.0 --output_dir ./ft/6 --evaluate_during_training --logging_steps 400 --save_steps 400 --warmup_percent 0.1 --hidden_dropout_prob 0.1 --overwrite_output --weight_decay 0.01 --n_process 1

python run_finetune.py --model_type dna --tokenizer_name=dna6 --model_name_or_path ./ft/6_finetune --task_name dnaprom --do_train --do_eval --data_dir sample_data/ft/m6a/6/A25 --max_seq_length 100 --per_gpu_eval_batch_size=10 --per_gpu_train_batch_size=10  --learning_rate 1e-4 --num_train_epochs 5.0 --output_dir ./ft/6 --evaluate_during_training --logging_steps 400 --save_steps 400 --warmup_percent 0.1 --hidden_dropout_prob 0.1 --overwrite_output --weight_decay 0.01 --n_process 1

python run_finetune.py --model_type dna --tokenizer_name=dna3 --model_name_or_path output3 --task_name dnaprom --do_train --do_eval --data_dir sample_data/ft/m6a/3/m_b --max_seq_length 100 --per_gpu_eval_batch_size=12 --per_gpu_train_batch_size=12  --learning_rate 1e-4 --num_train_epochs 5.0 --output_dir ./ft/3/m_b --evaluate_during_training --logging_steps 800 --save_steps 800 --warmup_percent 0.1 --hidden_dropout_prob 0.1 --overwrite_output --weight_decay 0.01 --n_process 1


export KMER=6
export MODEL_PATH=./ft/$KMER
export DATA_PATH=sample_data/ft/$KMER
export PREDICTION_PATH=./result/$KMER

python run_finetune.py --model_type dna --tokenizer_name=dna4 --model_name_or_path ./ft/4/m_b --task_name dnaprom --do_predict --data_dir sample_data/ft/m6a/4/m_b  --max_seq_length 100 --per_gpu_pred_batch_size=12  --output_dir ./ft/4/m_b --predict_dir ./result/4 --n_process 48


